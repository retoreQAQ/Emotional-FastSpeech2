import os
import json

import torch
import numpy as np

import hifigan
from model import FastSpeech2, ScheduledOptim


def get_model(args, configs, device, train=False):
    (preprocess_config, model_config, train_config) = configs

    model = FastSpeech2(preprocess_config, model_config).to(device)
    if args:
        if args.restore_step:
            ckpt_path = os.path.join(
                train_config["path"]["ckpt_path"],
                "{}.pth.tar".format(args.restore_step),
            )
            ckpt = torch.load(ckpt_path)
            model.load_state_dict(ckpt["model"])

    if train:
        scheduled_optim = ScheduledOptim(
            model, train_config, model_config, args.restore_step
        )
        if args.restore_step:
            scheduled_optim.load_state_dict(ckpt["optimizer"])
        model.train()
        return model, scheduled_optim

    model.eval()
    model.requires_grad_ = False
    return model

def get_model_finetune(args, configs, device, train=False):
    (preprocess_config, model_config, train_config) = configs

    model = FastSpeech2(preprocess_config, model_config).to(device)

    if args and args.restore_path:
        ckpt_path = args.restore_path
        print(f"æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {ckpt_path}")
        try:
            ckpt = torch.load(ckpt_path, map_location=device)
        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None

        # å®½æ¾åŠ è½½å‚æ•°
        ckpt_model_state = ckpt["model"]
        current_model_state = model.state_dict()

        matched_state = {}
        skipped = []
        for k, v in ckpt_model_state.items():
            if k not in current_model_state:
                print(f"è·³è¿‡å‚æ•° {k}: åœ¨å½“å‰æ¨¡å‹ä¸­ä¸å­˜åœ¨")
                skipped.append(k)
                continue

            if k == "speaker_emb.weight":
                pretrained, current = v.shape[0], current_model_state[k].shape[0]
                if pretrained <= current:
                    print(f"åˆå¹¶speaker_emb: é¢„è®­ç»ƒ={pretrained}, å½“å‰={current}")
                    new_weight = current_model_state[k].clone()
                    new_weight[:pretrained] = v  # ç”¨æ—§çš„å‰éƒ¨åˆ†å‚æ•°æ›¿æ¢
                    matched_state[k] = new_weight
                else:
                    print(f"æ— æ³•åŠ è½½speaker_emb: é¢„è®­ç»ƒ={pretrained}, å½“å‰={current}")
                    skipped.append(k)
                    continue
            elif k == "emotion_emb.weight" or k == "arousal_emb.weight" or k == "valence_emb.weight":
                # å¯¹äºæƒ…æ„Ÿç›¸å…³å‚æ•°ï¼Œå¦‚æœå½¢çŠ¶ä¸åŒ¹é…ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹çš„å‚æ•°
                print(f"ä½¿ç”¨å½“å‰æ¨¡å‹çš„æƒ…æ„Ÿå‚æ•°: {k}")
                matched_state[k] = current_model_state[k]
            elif v.shape == current_model_state[k].shape:
                matched_state[k] = v
            else:
                print(f"å‚æ•°å½¢çŠ¶ä¸åŒ¹é…: {k}, é¢„è®­ç»ƒ={v.shape}, å½“å‰={current_model_state[k].shape}")
                skipped.append(k)

        print(f"æˆåŠŸåŠ è½½ {len(matched_state)} ä¸ªå‚æ•°")
        print(f"åŠ è½½çš„å‚æ•°: {list(matched_state.keys())}")
        print(f"è·³è¿‡çš„å‚æ•°: {skipped}")

        model.load_state_dict(matched_state, strict=False)

    if train:
        scheduled_optim = ScheduledOptim(
            model, train_config, model_config,
            args.restore_step if args else 0
        )
        if args and args.restore_step and "optimizer" in ckpt:
            try:
                scheduled_optim.load_state_dict(ckpt["optimizer"])
                print("ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å¤±è´¥,å°†é‡æ–°åˆå§‹åŒ–: {e}")
        model.train()
        return model, scheduled_optim

    model.eval()
    model.requires_grad_ = False
    return model

def freeze_modules(model, freeze_encoder=True, freeze_decoder=True, freeze_variance=True, 
                  freeze_speaker=True, freeze_emotion=True, freeze_postnet=True):
    """å†»ç»“é€‰å®šçš„æ¨¡å—
    
    Args:
        model: FastSpeech2 æ¨¡å‹
        freeze_encoder: æ˜¯å¦å†»ç»“ç¼–ç å™¨å‚æ•°
        freeze_decoder: æ˜¯å¦å†»ç»“è§£ç å™¨å‚æ•°
        freeze_variance: æ˜¯å¦å†»ç»“æ–¹å·®é€‚é…å™¨å‚æ•°
        freeze_speaker: æ˜¯å¦å†»ç»“è¯´è¯äººåµŒå…¥å‚æ•°
        freeze_emotion: æ˜¯å¦å†»ç»“æƒ…æ„Ÿç›¸å…³å‚æ•°
        freeze_postnet: æ˜¯å¦å†»ç»“åå¤„ç†ç½‘ç»œå‚æ•°
    """
    # ç»Ÿè®¡å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = 0
    
    # if train_all:
    #     # å…¨è§£å†»æ¨¡å¼,æ‰€æœ‰å‚æ•°éƒ½å¯è®­ç»ƒ
    #     for p in model.parameters():
    #         p.requires_grad = True
    #     trainable_params = total_params
    #     print("\nğŸ”“ å…¨è§£å†»æ¨¡å¼: æ‰€æœ‰å‚æ•°éƒ½å¯è®­ç»ƒ")
    #     print(f"   - æ€»å‚æ•°æ•°é‡: {total_params}")
    #     print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params}")
    #     return
        
    # é»˜è®¤å†»ç»“æ‰€æœ‰å‚æ•°
    for p in model.parameters():
        p.requires_grad = False
    
    # æ ¹æ®é…ç½®è§£å†»ç‰¹å®šæ¨¡å—
    if not freeze_encoder:
        for p in model.encoder.parameters():
            p.requires_grad = True
        trainable_params += sum(p.numel() for p in model.encoder.parameters())
        print("ç¼–ç å™¨è§£å†»: ç¼–ç å™¨å‚æ•°å¯è®­ç»ƒ")
    else:
        print("ç¼–ç å™¨å†»ç»“: ç¼–ç å™¨å‚æ•°ä¸å¯è®­ç»ƒ")
        
    if not freeze_decoder:
        for p in model.decoder.parameters():
            p.requires_grad = True
        trainable_params += sum(p.numel() for p in model.decoder.parameters())
        print("è§£ç å™¨è§£å†»: è§£ç å™¨å‚æ•°å¯è®­ç»ƒ")
    else:
        print("è§£ç å™¨å†»ç»“: è§£ç å™¨å‚æ•°ä¸å¯è®­ç»ƒ")
        
    # æ–¹å·®é€‚é…å™¨
    if not freeze_variance:
        for p in model.variance_adaptor.parameters():
            p.requires_grad = True
        trainable_params += sum(p.numel() for p in model.variance_adaptor.parameters())
        print("æ–¹å·®é€‚é…å™¨è§£å†»: æ–¹å·®é€‚é…å™¨å‚æ•°å¯è®­ç»ƒ")
    else:
        print("æ–¹å·®é€‚é…å™¨å†»ç»“: æ–¹å·®é€‚é…å™¨å‚æ•°ä¸å¯è®­ç»ƒ")
    
    # è¯´è¯äººåµŒå…¥
    if not freeze_speaker and model.speaker_emb is not None:
        for p in model.speaker_emb.parameters():
            p.requires_grad = True
        trainable_params += sum(p.numel() for p in model.speaker_emb.parameters())
        print("è¯´è¯äººåµŒå…¥è§£å†»: è¯´è¯äººåµŒå…¥å‚æ•°å¯è®­ç»ƒ")
    else:
        print("è¯´è¯äººåµŒå…¥å†»ç»“: è¯´è¯äººåµŒå…¥å‚æ•°ä¸å¯è®­ç»ƒ")
        
    # æƒ…æ„Ÿç›¸å…³å‚æ•°
    if not freeze_emotion:
        if model.emotion_emb is not None:
            for p in model.emotion_emb.parameters():
                p.requires_grad = True
            trainable_params += sum(p.numel() for p in model.emotion_emb.parameters())
            print("æƒ…æ„ŸåµŒå…¥è§£å†»: æƒ…æ„ŸåµŒå…¥å‚æ•°å¯è®­ç»ƒ")
            
        if model.arousal_emb is not None:
            for p in model.arousal_emb.parameters():
                p.requires_grad = True
            trainable_params += sum(p.numel() for p in model.arousal_emb.parameters())
            print("å”¤é†’åº¦åµŒå…¥è§£å†»: å”¤é†’åº¦åµŒå…¥å‚æ•°å¯è®­ç»ƒ")
            
        if model.valence_emb is not None:
            for p in model.valence_emb.parameters():
                p.requires_grad = True
            trainable_params += sum(p.numel() for p in model.valence_emb.parameters())
            print("æ•ˆä»·åµŒå…¥è§£å†»: æ•ˆä»·åµŒå…¥å‚æ•°å¯è®­ç»ƒ")
    else:
        print("æƒ…æ„Ÿç›¸å…³å‚æ•°å†»ç»“: æƒ…æ„Ÿã€å”¤é†’åº¦ã€æ•ˆä»·åµŒå…¥å‚æ•°ä¸å¯è®­ç»ƒ")
    
    # åå¤„ç†ç½‘ç»œ
    if not freeze_postnet:
        for p in model.mel_linear.parameters():
            p.requires_grad = True
        for p in model.postnet.parameters():
            p.requires_grad = True
        trainable_params += sum(p.numel() for p in model.mel_linear.parameters())
        trainable_params += sum(p.numel() for p in model.postnet.parameters())
        print("åå¤„ç†ç½‘ç»œè§£å†»: æ¢…å°”çº¿æ€§å±‚å’Œåå¤„ç†ç½‘ç»œå‚æ•°å¯è®­ç»ƒ")
    else:
        print("åå¤„ç†ç½‘ç»œå†»ç»“: æ¢…å°”çº¿æ€§å±‚å’Œåå¤„ç†ç½‘ç»œå‚æ•°ä¸å¯è®­ç»ƒ")
    
    # æ‰“å°å‚æ•°ç»Ÿè®¡
    print(f"\nå‚æ•°ç»Ÿè®¡:")
    print(f"   - æ€»å‚æ•°æ•°é‡: {total_params}")
    print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params}")
    print(f"   - å†»ç»“å‚æ•°: {total_params - trainable_params}")
    print(f"   - å¯è®­ç»ƒæ¯”ä¾‹: {trainable_params/total_params*100:.2f}%")


def get_param_num(model):
    num_param = sum(param.numel() for param in model.parameters())
    return num_param


def get_vocoder(config, device):
    name = config["vocoder"]["model"]
    speaker = config["vocoder"]["speaker"]

    if name == "MelGAN":
        if speaker == "LJSpeech":
            vocoder = torch.hub.load(
                "descriptinc/melgan-neurips", "load_melgan", "linda_johnson"
            )
        elif speaker == "universal":
            vocoder = torch.hub.load(
                "descriptinc/melgan-neurips", "load_melgan", "multi_speaker"
            )
        vocoder.mel2wav.eval()
        vocoder.mel2wav.to(device)
    elif name == "HiFi-GAN":
        with open("hifigan/config.json", "r") as f:
            config = json.load(f)
        config = hifigan.AttrDict(config)
        vocoder = hifigan.Generator(config)
        if speaker == "LJSpeech":
            ckpt = torch.load("hifigan/generator_LJSpeech.pth.tar")
        elif speaker == "universal":
            ckpt = torch.load("hifigan/generator_universal.pth.tar")
        vocoder.load_state_dict(ckpt["generator"])
        vocoder.eval()
        vocoder.remove_weight_norm()
        vocoder.to(device)

    return vocoder


def vocoder_infer(mels, vocoder, model_config, preprocess_config, lengths=None):
    name = model_config["vocoder"]["model"]
    with torch.no_grad():
        if name == "MelGAN":
            wavs = vocoder.inverse(mels / np.log(10))
        elif name == "HiFi-GAN":
            wavs = vocoder(mels).squeeze(1)

    wavs = (
        wavs.cpu().numpy()
        * preprocess_config["preprocessing"]["audio"]["max_wav_value"]
    ).astype("int16")
    wavs = [wav for wav in wavs]

    for i in range(len(mels)):
        if lengths is not None:
            wavs[i] = wavs[i][: lengths[i]]

    return wavs
